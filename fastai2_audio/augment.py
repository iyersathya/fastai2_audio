# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/01_augment.ipynb (unless otherwise specified).

__all__ = ['RemoveSilence', 'Resample', 'CropSignal', 'shift_signal', 'SignalShifter', 'AddNoise', 'ChangeVolume',
           'SignalCutout', 'SignalLoss', 'DownmixMono', 'CropTime', 'MaskFreq', 'MaskFreqRandom', 'MaskTime',
           'MaskTimeRandom', 'SGRoll', 'Delta', 'TfmResize']

# Cell
from fastai2.data.all import *
from .core import *
from fastai2.vision.augment import *

# Cell
import torch.nn
from torch import stack, zeros_like as t0, ones_like as t1
from torch.distributions.bernoulli import Bernoulli
from librosa.effects import split
from dataclasses import asdict
from scipy.signal import resample_poly

from scipy.ndimage.interpolation import shift
import librosa
import colorednoise as cn

# Cell
mk_class('RemoveType', **{o:o.lower() for o in ['Trim', 'All', 'Split']},
         doc="All methods of removing silence as attributes to get tab-completion and typo-proofing")

# Cell
class RemoveSilence(Transform):
    "Split signal at points of silence greater than 2*pad_ms"
    def __init__(self, remove_type=RemoveType.Trim, threshold=20, pad_ms=20,**kwargs):
        super().__init__(**kwargs)
        self.remove_type = remove_type
        self.threshold = threshold
        self.pad_ms = pad_ms

    def _merge_splits(self,splits, pad):
        clip_end = splits[-1][1]
        merged = []
        i=0
        while i < len(splits):
            start = splits[i][0]
            while splits[i][1] < clip_end and splits[i][1] + pad >= splits[i+1][0] - pad:
                i += 1
            end = splits[i][1]
            merged.append(np.array([max(start-pad, 0), min(end+pad, clip_end)]))
            i+=1
        return np.stack(merged)

    def encodes(self,ai:AudioTensor)->AudioTensor:
        if self.remove_type is None: return ai
        padding = int(self.pad_ms/1000*ai.sr)
        if(padding > ai.nsamples): return ai
        splits = split(ai.numpy(), top_db=self.threshold, hop_length=padding)
        if self.remove_type == "split":
            sig =  [ai[:,(max(a-padding,0)):(min(b+padding,ai.nsamples))]
                    for (a, b) in self._merge_splits(splits, padding)]
        elif self.remove_type == "trim":
            sig = [ai[:,(max(splits[0, 0]-padding,0)):splits[-1, -1]+padding]]
        elif self.remove_type == "all":
            sig = [torch.cat([ai[:,(max(a-padding,0)):(min(b+padding,ai.nsamples))]
                              for (a, b) in self._merge_splits(splits, padding)], dim=1)]
        else:
            raise ValueError(f"Valid options for silence removal are None, 'split', 'trim', 'all' not '{remove_type}'.")
        ai.data = torch.cat(sig, dim=-1)
        return ai


# Cell
class Resample(Transform):
    "Resample using faster polyphase technique and avoiding FFT computation"
    def __init__(self, sr_new, **kwargs):
        super().__init__(**kwargs)
        self.sr_new = sr_new

    def encodes(self, ai:AudioTensor)->AudioTensor:
        if(ai.sr == self.sr_new): return ai
        sig_np = ai.numpy()
        sr_gcd = math.gcd(ai.sr, self.sr_new)
        resampled = resample_poly(sig_np, int(self.sr_new/sr_gcd), int(ai.sr/sr_gcd), axis=-1)
        ai.data = torch.from_numpy(resampled.astype(np.float32))
        ai.sr = self.sr_new
        return ai

# Cell
mk_class('AudioPadType', **{o:o.lower() for o in ['Zeros', 'Zeros_After', 'Repeat']},
         doc="All methods of padding audio as attributes to get tab-completion and typo-proofing")

# Cell
class CropSignal(Transform):
    "Crops signal to be length specified in ms by duration, padding if needed"
    def __init__(self, duration, pad_mode=AudioPadType.Zeros,**kwargs):
        super().__init__(**kwargs)
        self.duration = duration
        self.pad_mode = pad_mode

    def encodes(self, ai:(AudioTensor)) -> AudioTensor:
        sig = ai.data
        orig_samples = ai.nsamples
        crop_samples = int((self.duration/1000)*ai.sr)
        if orig_samples == crop_samples: return ai
        elif orig_samples < crop_samples:
            ai.data = _tfm_pad_signal(sig, crop_samples, pad_mode=self.pad_mode)
        else:
            crop_start = random.randint(0, int(orig_samples-crop_samples))
            ai.data = sig[:,crop_start:crop_start+crop_samples]
        return ai

# Cell
def _tfm_pad_signal(sig, width, pad_mode=AudioPadType.Zeros):
    "Pad spectrogram to specified width, using specified pad mode"
    c,x = sig.shape
    pad_m = pad_mode.lower()
    if pad_m in ["zeros", "zeros_after"]:
        zeros_front = random.randint(0, width-x) if pad_m == "zeros" else 0
        pad_front = torch.zeros((c, zeros_front))
        pad_back = torch.zeros((c, width-x-zeros_front))
        return torch.cat((pad_front, sig, pad_back), 1)
    elif pad_m == "repeat":
        repeats = width//x + 1
        return sig.repeat(1,repeats)[:,:width]
    else:
        raise ValueError(f"pad_mode {pad_m} not currently supported, only 'zeros', 'zeros_after', or 'repeat'")

# Cell
def _shift(sig, s):
    if s == 0: return sig
    out = torch.zeros_like(sig)
    if  s < 0: out[...,:s] = sig[...,-s:]
    else: out[...,s:] = sig[...,:-s]
    return out

def shift_signal(t:torch.Tensor, shift, roll):
    #refactor 2nd half of this statement to just take and roll the final axis
    if roll: t.data = torch.from_numpy(np.roll(t.numpy(), shift, axis=-1))
    else   : t.data = _shift(t, shift)
    return t

# Cell
class SignalShifter(RandTransform):
    def __init__(self, p=0.5, max_pct= 0.2, max_time=None, direction=0, roll=False):
        if direction not in [-1, 0, 1]: raise ValueError("Direction must be -1(left) 0(bidirectional) or 1(right)")
        store_attr(self, "max_pct,max_time,direction,roll")
        super().__init__(p=p)

    def before_call(self, b, split_idx):
        super().before_call(b, split_idx)
        self.shift_factor = random.uniform(-1, 1)
        if self.direction != 0: self.shift_factor = self.direction*abs(self.shift_factor)

    def encodes(self, ai:AudioTensor):
        if self.max_time is None: s = self.shift_factor*self.max_pct*ai.nsamples
        else:                     s = self.shift_factor*self.max_time*ai.sr
        ai.data = shift_signal(ai.data, int(s), self.roll)
        return ai

    def encodes(self, sg:AudioSpectrogram):
        if self.max_time is None: s = self.shift_factor*self.max_pct*sg.width
        else:                     s = self.shift_factor*self.max_time*sg.sr
        return shift_signal(sg, int(s), self.roll)

# Cell
mk_class('NoiseColor', **{o:i-2 for i,o in enumerate(['Violet', 'Blue', 'White', 'Pink', 'Brown'])},
         doc="All possible colors of noise as attributes to get tab-completion and typo-proofing")

# Cell
class AddNoise(Transform):
    def __init__(self,noise_level=0.05, color=NoiseColor.White,**kwargs):
        super().__init__(**kwargs)
        self.noise_level = noise_level
        self.color = color

    def encodes(self,ai: AudioTensor)->AudioTensor:
        # if it's white noise, implement our own for speed
        if self.color==0: noise = torch.randn_like(ai.data)
        else:        noise = torch.from_numpy(cn.powerlaw_psd_gaussian(exponent=self.color, size=ai.nsamples)).float()
        scaled_noise = noise * ai.data.abs().mean() * self.noise_level
        ai.data += scaled_noise
        return ai


# Cell
@patch
def apply_gain(ai:AudioTensor, gain):
    ai.data *= gain
    return ai

# Cell
class ChangeVolume(RandTransform):
    def __init__(self, p=0.5, lower=0.5, upper=1.5):
        self.lower, self.upper = lower, upper
        super().__init__(p=p)

    def before_call(self, b, split_idx):
        super().before_call(b, split_idx)
        self.gain = random.uniform(self.lower, self.upper)

    def encodes(self, ai:AudioTensor): return apply_gain(ai, self.gain)

# Cell
@patch
def cutout(ai:AudioTensor, cut_pct):
    mask = torch.zeros(int(ai.nsamples*cut_pct))
    mask_start = random.randint(0,ai.nsamples-len(mask))
    ai.data[:,mask_start:mask_start+len(mask)] = mask
    return ai

# @patch
# def cutout(sg:AudioSpectrogram, cut_pct):

# Cell
class SignalCutout(RandTransform):
    def __init__(self, p=0.5, max_cut_pct=0.15):
        self.max_cut_pct = max_cut_pct
        super().__init__(p=p)

    def before_call(self, b, split_idx):
        super().before_call(b, split_idx)
        self.cut_pct = random.uniform(0, self.max_cut_pct)

    def encodes(self, ai:AudioTensor): return cutout(ai, self.cut_pct)

# Cell
@patch
def lose_signal(ai:AudioTensor, loss_pct):
    mask = (torch.rand_like(ai.data[0])>loss_pct).float()
    ai.data[...,:] *= mask
    return ai

# Cell
class SignalLoss(RandTransform):
    def __init__(self, p=0.5, max_loss_pct = 0.15):
        self.max_loss_pct = max_loss_pct
        super().__init__(p=p)

    def before_call(self, b, split_idx):
        super().before_call(b, split_idx)
        self.loss_pct = random.uniform(0, self.max_loss_pct)

    def encodes(self, ai:AudioTensor): return lose_signal(ai, self.loss_pct)

# Cell
class DownmixMono(Transform):
    # downmixMono was removed from torchaudio, we now just take the mean across channels
    # this works for both batches and individual items
    def __init__(self,**kwargs):
        super().__init__(**kwargs)

    def encodes(self, ai: AudioTensor)->AudioTensor:
        """Randomly replaces amplitude of signal with 0. Simulates analog info loss"""
        downmixed = ai.data.contiguous().mean(-2).unsqueeze(-2)
        return AudioTensor(downmixed, ai.sr)

# Cell
class CropTime(Transform):
    "Random crops full spectrogram to be length specified in ms by crop_duration"
    def __init__(self,duration, pad_mode=AudioPadType.Zeros,**kwargs):
        store_attr(self, "duration,pad_mode")
        super().__init__(**kwargs)

    def encodes(self, sg:AudioSpectrogram)->AudioSpectrogram:
        sr, hop = sg.sr, sg.hop_length
        w_crop = int((sr*self.duration)/(1000*hop))+1
        w_sg   = sg.shape[-1]
        if     w_sg == w_crop: sg_crop = sg
        elif   w_sg <  w_crop: sg_crop = _tfm_pad_spectro(sg, w_crop, pad_mode=self.pad_mode)
        else:
            crop_start = random.randint(0, int(w_sg - w_crop))
            sg_crop = sg[:,:,crop_start:crop_start+w_crop]
            sg_crop.sample_start = int(crop_start*hop)
            sg_crop.sample_end   = sg_crop.sample_start + int(self.duration*sr)
        sg.data = sg_crop
        return sg


# Cell
def _tfm_pad_spectro(sg, width, pad_mode=AudioPadType.Zeros):
    "Pad spectrogram to specified width, using specified pad mode"
    c,y,x = sg.shape
    pad_m = pad_mode.lower()
    if pad_m in ["zeros", "zeros_after"]:
        padded = torch.zeros((c,y,width))
        start = random.randint(0, width-x) if pad_m == "zeros" else 0
        padded[:,:,start:start+x] = sg.data
        return padded
    elif pad_m == "repeat":
        repeats = width//x + 1
        return sg.repeat(1,1,repeats)[:,:,:width]
    else:
        raise ValueError(f"pad_mode {pad_m} not currently supported, only 'zeros', 'zeros_after', or 'repeat'")

# Cell
class MaskFreq(Transform):
    "Google SpecAugment time masking from https://arxiv.org/abs/1904.08779."
    def __init__(self,num_masks=1, size=20, start=None, val=None, **kwargs):
        store_attr(self,"num_masks,size,start,val")
        super().__init__(**kwargs)

    def encodes(self, sg:AudioSpectrogram)->AudioSpectrogram:
        start = self.start
        channel_mean = sg.contiguous().view(sg.size(0), -1).mean(-1)[:,None,None]
        mask_val = channel_mean if self.val is None else self.val
        c, y, x = sg.shape
        for _ in range(self.num_masks):
            mask = torch.ones(self.size, x) * mask_val
            if start is None: start= random.randint(0, y-self.size)
            if not 0 <= start <= y-self.size:
                raise ValueError(f"Start value '{start}' out of range for AudioSpectrogram of shape {sg.shape}")
            sg[:,start:start+self.size,:] = mask
        return sg


# Cell
class MaskFreqRandom(RandTransform):
    def __init__(self,num_masks=1, min_size=0,max_size=20, start=None, val=None, **kwargs):
        store_attr(self,"num_masks,min_size,max_size,start,val")
        super().__init__(**kwargs)

    def before_call(self, b, split_idx):
        if not split_idx:
            if self.val:
                self.val = random.randint(0, val)
            self.size = random.randint(self.min_size, self.max_size)
            self.start = None
        else:
            self.size = (self.max_size- self.min_size)//2


    def encodes(self, sg:AudioSpectrogram)->AudioSpectrogram:
        start = self.start
        channel_mean = sg.contiguous().view(sg.size(0), -1).mean(-1)[:,None,None]
        mask_val = channel_mean if self.val is None else self.val
        c, y, x = sg.shape
        for _ in range(self.num_masks):
            mask = torch.ones(self.size, x) * mask_val
            if start is None: start= random.randint(0, y-self.size)
            if not 0 <= start <= y-self.size:
                raise ValueError(f"Start value '{start}' out of range for AudioSpectrogram of shape {sg.shape}")
            sg[:,start:start+self.size,:] = mask
        return sg

# Cell
class MaskTime(Transform):
    def __init__(self,num_masks=1, size=20, start=None, val=None, **kwargs):
        store_attr(self,"num_masks,size,start,val")
        super().__init__(**kwargs)

    def encodes(self,sg:AudioSpectrogram)->AudioSpectrogram:
        sg.data = torch.einsum('...ij->...ji', sg)
        sg.data = MaskFreq(self.num_masks, self.size, self.start, self.val)(sg)
        sg.data = torch.einsum('...ij->...ji', sg)
        return sg


# Cell
class MaskTimeRandom(RandTransform):
    def __init__(self,num_masks=1, min_size=0,max_size=20, start=None, val=None, **kwargs):
        store_attr(self,"num_masks,min_size,max_size,start,val")
        super().__init__(**kwargs)

    def before_call(self, b, split_idx):
        if not split_idx:
            if self.val:
                self.val = random.randint(0, val)
            self.size = random.randint(self.min_size, self.max_size)
            self.start = None
        else:
            self.size = (self.max_size- self.min_size)//2

    def encodes(self,sg:AudioSpectrogram)->AudioSpectrogram:
        sg.data = torch.einsum('...ij->...ji', sg)
        sg.data = MaskFreq(self.num_masks, self.size, self.start, self.val)(sg)
        sg.data = torch.einsum('...ij->...ji', sg)
        return sg


# Cell
class SGRoll(Transform):
    "Shifts spectrogram along x-axis wrapping around to other side"
    def __init__(self,max_shift_pct=0.5, direction=0, **kwargs):
        if int(direction) not in [-1, 0, 1]:
            raise ValueError("Direction must be -1(left) 0(bidirectional) or 1(right)")
        store_attr(self,'max_shift_pct,direction')
        super().__init__(**kwargs)

    def encodes(self,sg:AudioSpectrogram)->AudioSpectrogram:
        direction = random.choice([-1, 1]) if self.direction == 0 else self.direction
        w = sg.shape[-1]
        roll_by = int(w*random.random()*self.max_shift_pct*direction)
        sg.data = sg.roll(roll_by, dims=-1)
        return sg


# Cell
def _torchdelta(sg:AudioSpectrogram, order=1, width=9):
    "Converts to numpy, takes delta and converts back to torch, needs torchification"
    if(sg.shape[1] < width):
        raise ValueError(f'''Delta not possible with current settings, inputs must be wider than
        {width} columns, try setting max_to_pad to a larger value to ensure a minimum width''')
    return AudioSpectrogram(torch.from_numpy(librosa.feature.delta(sg.numpy(), order=order, width=width)))

# Cell
class Delta(Transform):
    def __init__(self, width=9,**kwargs):
        self.width = width
        self.td = partial(_torchdelta, width=width)

    def encodes(self,sg:AudioSpectrogram)->AudioSpectrogram:
        new_channels = [torch.stack([c, self.td(c, order=1), self.td(c, order=2)]) for c in sg]
        sg.data = torch.cat(new_channels, dim=0)
        return sg

# Cell
class TfmResize(Transform):
    "Temporary fix to allow image resizing transform"
    def __init__(self,size, interp_mode="bilinear", **kwargs):
        store_attr(self,"size,interp_mode")
        super().__init__(**kwargs)

    def encodes(self,sg:AudioSpectrogram)->AudioSpectrogram:
        size = self.size
        if isinstance(size, int): size = (size, size)
        c,y,x = sg.shape
        sg.data = F.interpolate(sg.unsqueeze(0), size=size, mode=self.interp_mode, align_corners=False).squeeze(0)
        return sg